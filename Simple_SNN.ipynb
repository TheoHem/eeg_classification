{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import spikegen\n",
    "from snntorch import surrogate\n",
    "from prepare_dataset import *\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, beta=0.9):\n",
    "        super(SNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=surrogate.fast_sigmoid())\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=surrogate.fast_sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the SNN using both spikes and membrane potentials.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, input_size, num_timesteps)\n",
    "        \n",
    "        Returns:\n",
    "        - spk_rec: Spikes recorded over timesteps.\n",
    "        - mem_rec: Membrane potentials recorded over timesteps.\n",
    "        \"\"\"\n",
    "        # Initialize membrane potentials\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record spikes and membrane potentials\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(x.size(2)):  # Iterate over time steps\n",
    "            cur_input = x[:, :, step]  # Input at the current timestep\n",
    "\n",
    "            # First layer\n",
    "            mem1, spk1 = self.lif1(self.fc1(cur_input), mem1)\n",
    "\n",
    "            # Second layer\n",
    "            mem2, spk2 = self.lif2(self.fc2(spk1), mem2)\n",
    "\n",
    "            # Record spikes and membrane potentials\n",
    "            spk_rec.append(spk2)\n",
    "            mem_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk_rec, dim=2), torch.stack(mem_rec, dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All of the functions\n",
    "\"\"\"\n",
    "\n",
    "def compute_class_weights(train_loader):\n",
    "    class_counts = np.zeros(4)  # Assuming 4 classes: Up, Down, Right, Left\n",
    "\n",
    "    # Count occurrences of each class in the training dataset\n",
    "    for _, labels in train_loader:\n",
    "        for label in labels.cpu().numpy():\n",
    "            class_counts[label] += 1\n",
    "\n",
    "    # Inverse frequency\n",
    "    class_weights = 1. / class_counts\n",
    "    class_weights = class_weights / np.sum(class_weights)  # Normalize to sum to 1\n",
    "\n",
    "    # Convert to tensor\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "def plot_label_distribution(train_loader, val_loader, test_loader):\n",
    "    \"\"\"\n",
    "    Plot label distribution for the train, validation, and test DataLoaders.\n",
    "\n",
    "    Parameters:\n",
    "    - train_loader: DataLoader for the training set.\n",
    "    - val_loader: DataLoader for the validation set.\n",
    "    - test_loader: DataLoader for the test set.\n",
    "    \"\"\"\n",
    "    # Function to collect label frequencies\n",
    "    def collect_labels(loader):\n",
    "        labels = []\n",
    "        for _, label_batch in loader:\n",
    "            labels.extend(label_batch.cpu().numpy())\n",
    "        return np.array(labels)\n",
    "\n",
    "    # Collect labels from each dataset\n",
    "    train_labels = collect_labels(train_loader)\n",
    "    val_labels = collect_labels(val_loader)\n",
    "    test_labels = collect_labels(test_loader)\n",
    "\n",
    "    # Plot label distribution\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plotting for each dataset\n",
    "    ax[0].hist(train_labels, bins=np.arange(5)-0.5, edgecolor=\"black\", alpha=0.7)\n",
    "    ax[0].set_title(\"Training Set Label Distribution\")\n",
    "    ax[0].set_xlabel(\"Class\")\n",
    "    ax[0].set_ylabel(\"Frequency\")\n",
    "    ax[0].set_xticks([0, 1, 2, 3])\n",
    "    ax[0].set_xticklabels([\"Up\", \"Down\", \"Right\", \"Left\"])\n",
    "\n",
    "    ax[1].hist(val_labels, bins=np.arange(5)-0.5, edgecolor=\"black\", alpha=0.7)\n",
    "    ax[1].set_title(\"Validation Set Label Distribution\")\n",
    "    ax[1].set_xlabel(\"Class\")\n",
    "    ax[1].set_ylabel(\"Frequency\")\n",
    "    ax[1].set_xticks([0, 1, 2, 3])\n",
    "    ax[1].set_xticklabels([\"Up\", \"Down\", \"Right\", \"Left\"])\n",
    "\n",
    "    ax[2].hist(test_labels, bins=np.arange(5)-0.5, edgecolor=\"black\", alpha=0.7)\n",
    "    ax[2].set_title(\"Test Set Label Distribution\")\n",
    "    ax[2].set_xlabel(\"Class\")\n",
    "    ax[2].set_ylabel(\"Frequency\")\n",
    "    ax[2].set_xticks([0, 1, 2, 3])\n",
    "    ax[2].set_xticklabels([\"Up\", \"Down\", \"Right\", \"Left\"])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_data_loaders(dataset, train_frac=0.7, val_frac=0.2, test_frac=0.1, batch_size=16, shuffle=True):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training, validation, and test sets, and creates DataLoaders.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: The full dataset (TensorDataset).\n",
    "    - train_frac: Fraction of the dataset to use for training.\n",
    "    - val_frac: Fraction of the dataset to use for validation.\n",
    "    - test_frac: Fraction of the dataset to use for testing.\n",
    "    - batch_size: The batch size for the DataLoaders.\n",
    "    - shuffle: Whether to shuffle the dataset.\n",
    "    \n",
    "    Returns:\n",
    "    - train_loader: DataLoader for the training set.\n",
    "    - val_loader: DataLoader for the validation set.\n",
    "    - test_loader: DataLoader for the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get dataset size\n",
    "    total_size = len(dataset)\n",
    "    \n",
    "    # Calculate sizes for each subset\n",
    "    train_size = int(total_size * train_frac)\n",
    "    val_size = int(total_size * val_frac)\n",
    "    test_size = total_size - train_size - val_size  # Remaining size for test set\n",
    "    \n",
    "    # Split the dataset into train, val, and test subsets\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_snn(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience=5):\n",
    "    \"\"\"\n",
    "    Train and validate the SNN model, returning training and validation losses.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The SNN model to train.\n",
    "    - train_loader: DataLoader for the training set.\n",
    "    - val_loader: DataLoader for the validation set.\n",
    "    - criterion: The loss function.\n",
    "    - optimizer: The optimizer to use.\n",
    "    - num_epochs: The number of epochs to train.\n",
    "    - device: The device ('cuda' or 'cpu') where the model is running.\n",
    "    \n",
    "    Returns:\n",
    "    - train_losses: List of training losses for each epoch.\n",
    "    - val_losses: List of validation losses for each epoch.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch_idx, (spikes, labels) in enumerate(train_loader):\n",
    "            spikes, labels = spikes.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            spk_rec, mem_rec = model(spikes)\n",
    "            num_timesteps = spk_rec.size(2)\n",
    "\n",
    "            # Compute loss using membrane potential across all timesteps\n",
    "            total_loss = torch.zeros(1, device=device)\n",
    "            for step in range(num_timesteps):\n",
    "                total_loss += criterion(mem_rec[:, :, step].squeeze(1), labels)  # Use membrane potential for loss\n",
    "\n",
    "            total_loss /= num_timesteps  # Normalize by timesteps\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += total_loss.item()\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                  f\"Train Loss: {total_loss.item():.4f}\", end='\\r')\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for spikes, labels in val_loader:\n",
    "                spikes, labels = spikes.to(device), labels.to(device)\n",
    "\n",
    "                spk_rec, mem_rec = model(spikes)\n",
    "                num_timesteps = spk_rec.size(2)\n",
    "\n",
    "                # Compute validation loss using membrane potential across all timesteps\n",
    "                total_loss = torch.zeros(1, device=device)\n",
    "                for step in range(num_timesteps):\n",
    "                    total_loss += criterion(mem_rec[:, :, step].squeeze(1), labels)  # Use membrane potential for loss\n",
    "\n",
    "                total_loss /= num_timesteps\n",
    "                total_val_loss += total_loss.item()\n",
    "\n",
    "                # Calculate accuracy based on spike count\n",
    "                spike_count = spk_rec.sum(dim=2)  # Sum spikes across all timesteps\n",
    "                _, predicted = torch.max(spike_count, 1)  # Use spike count for accuracy\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Validation Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def test_snn(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Test the SNN model and return performance metrics including accuracy, precision, recall, F1 score, and confusion matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained SNN model.\n",
    "    - test_loader: DataLoader for the test set.\n",
    "    - device: The device ('cuda' or 'cpu') where the model is running.\n",
    "    \n",
    "    Returns:\n",
    "    - accuracy: The accuracy of the model on the test set.\n",
    "    - precision: Precision score.\n",
    "    - recall: Recall score.\n",
    "    - f1_score: F1 score.\n",
    "    - confusion_matrix: The confusion matrix.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for spikes, labels in test_loader:\n",
    "            spikes, labels = spikes.to(device), labels.to(device)\n",
    "            spk_rec, mem_rec = model(spikes)\n",
    "            outputs = mem_rec[:, :, -1]  # Use last timestep for classification\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * total_correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Classification report with zero_division to handle undefined metrics\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=[\"Up\", \"Down\", \"Right\", \"Left\"], zero_division=1))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Up\", \"Down\", \"Right\", \"Left\"], yticklabels=[\"Up\", \"Down\", \"Right\", \"Left\"])\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"inner-speech-recognition\"\n",
    "n_s = 1\n",
    "model_type = \"SNN\"\n",
    "fs = 256\n",
    "Tstart = 1.5\n",
    "Tend = 3.5\n",
    "encoding = \"poisson\"\n",
    "\n",
    "X, Y = create_dataset(root_dir, n_s, model_type, fs, \n",
    "                   Tstart, Tend, encoding)\n",
    "# X shape (trials, channels, time)\n",
    "\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)  # Convert to tensor, adjust dtype if needed\n",
    "Y = torch.tensor(Y, dtype=torch.long)    # Ensure labels are integers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(X, Y)\n",
    "#train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "\n",
    "train_loader, val_loader, test_loader = create_data_loaders(train_data, train_frac=0.7, val_frac=0.2, test_frac=0.1, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "print(f'Batches in train, val, and test loader: {len(train_loader)}, {len(val_loader)}, {len(test_loader)}')\n",
    "print(f'Samples in train, val, and test loader: {len(train_loader) * 16}, {len(val_loader)*16 }, {len(test_loader)*16}')\n",
    "plot_label_distribution(train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X.shape[1]  # Number of channels\n",
    "hidden_size = 100  # Hidden layer size\n",
    "num_classes = len(torch.unique(Y))  # Number of classes\n",
    "\n",
    "\n",
    "snn_model = SNN(input_size, hidden_size, num_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(snn_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "train_losses, val_losses = train_snn(snn_model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label='Training Loss', color='blue', marker='o')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss', color='orange', marker='x')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_snn(snn_model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
